{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Suite\n",
    "### Introduction\n",
    "In order to validate your data, Great Expectations is a package that offers a battery-included set of logic to get up-and-running fast. Fully figuring out how Great Expectations works and applying it to your project, however, can be somewhat involved.\n",
    "\n",
    "Therefore, this bootstrapped project helps you with that by making a few choices for you and offering scripts and notebooks to get you up and running fast. The choices already made here are:\n",
    "- Great Expectations output will be stored on S3\n",
    "- The rendered Data Docs site will be stored on S3\n",
    "- You will write your own data loading logic to read data into memory as a pandas DataFrame\n",
    "- You will write your own set of expectations to test the quality of this data\n",
    "- The validation logic will be deployed as Docker container via AWS Lambda\n",
    "\n",
    "To set you up for the above, the configuration file had you enter parameters that will be used throughout this project. Assuming these were properly set, you can now continue to set up your expectation suite!\n",
    "\n",
    "### Description\n",
    "This notebook can be used to generate a so-called expectation suite that can be used to run validations against your data. An expectation suite is Great Expectations jargon for a collection of expectations (or tests) that you want to run your data against. \n",
    "\n",
    "This notebook helps you to set up such a set of expectations, by loading a batch of data and writing expectations that can be run on it. After doing so, this set of expectations will be saved as an expectation suite.\n",
    "\n",
    "In the last step, this expectation suite will be connected to a checkpoint, which is an object which can be called by validation logic to run new data batches against. Hence, after setting up an expectation suite and checkpoint using this notebook, the next step is to finalize the AWS lambda function found in lambda_function.py and deploy that on AWS as a Docker image. To assist with these steps Terraform configurations, a Dockerfile and a bash script ('build_image_store_on_ecr.sh) were automatically generated.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "#### Virtual environment\n",
    "In order to run the logic contained within this notebook, make sure that it was started up from a virtual environment that contained all required Python dependencies. The easiest way to assure this is to first make a virtual environment for the project and then call initialize_project.py from within it.\n",
    "\n",
    "To create a new virtual environment, e.g. for python 3.8, and installing the packages that can be found in the requirements.txt file, the commands are:\n",
    "```sh\n",
    "conda create --name <virtual_environment_name> python=3.8\n",
    "conda activate <virtual_environment_name>\n",
    "pip install -r </path/to/requirements.txt>\n",
    "```\n",
    "\n",
    "#### S3 buckets\n",
    "For testing, Great Expectations is configured to interact with S3 buckets on AWS. To be able to run all the code in this notebook the **store_bucket** and **site_bucket** as configured in the testing_config.yml, must be provisioned.\n",
    "\n",
    "To do so, auto-generated Terraform files can be found in the *terraform* directory of this project. To use these configurations to generate S3 buckets, open a terminal in this directory and run the following commands:\n",
    "\n",
    "```bash\n",
    "# Initialize Terraform\n",
    "terraform init\n",
    "\n",
    "# Generate deployment plan, enter yes at the prompt if the plan is correct\n",
    "terraform apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and configurations\n",
    "In the cell below, packages are imported and configurations are loaded from the project_config.yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Imports\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "import boto3\n",
    "from supporting_functions import (TestingConfiguration,\n",
    "                                  get_file_keys_from_s3,\n",
    "                                  checkpoint_without_datadocs_update, \n",
    "                                  print_ge_site_link,\n",
    "                                  tutorial_invoke_lambdas)\n",
    "from supporting_functions import load_csv_from_s3 as load_data\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# -- Load parameters from configuration file\n",
    "test_config = TestingConfiguration(\"project_config.yml\")\n",
    "test_config.load_config()\n",
    "\n",
    "# -- Constant parameters\n",
    "PATH_TUTORIAL_DATA = \"data/\"\n",
    "ROW_COUNT_DELTA = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of objects\n",
    "Next, objects required to interact with AWS and the Great Expectations configurations are initialized.\n",
    "\n",
    "Specifically for Great Expectations, the `context` object is initialized. By loading the GE configuration file (found in the subdirectory great_expectations), this object stores important parameters for you to interact with GE and automatically interact with S3 buckets for storing output, pulling checkpoints and generating Data Docs sites. For more GE information, check out [their website](https://docs.greatexpectations.io/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1. Initialize GE and S3 objects\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket = boto3.resource(\"s3\").Bucket(test_config.data_bucket)\n",
    "context = ge.data_context.DataContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading tutorial data to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in os.listdir(PATH_TUTORIAL_DATA):\n",
    "    # -- Set path to file\n",
    "    path_file = os.path.abspath(PATH_TUTORIAL_DATA+dataset)\n",
    "\n",
    "    # -- Upload to S3\n",
    "    bucket.meta.client.upload_file(\n",
    "        Filename=path_file, \n",
    "        Bucket=test_config.data_bucket, \n",
    "        Key=PATH_TUTORIAL_DATA+dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "Now, logic must be written for loading data. The easiest way to do so is to write a new function for this that you store in supporting_functions.py, so that it can be used both in this notebook and in the lambda function.\n",
    "\n",
    "If you have your data locally as a CSV file, such a function could be as simple as:\n",
    "\n",
    "```python\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    df_batch = pd.read_csv(path)\n",
    "\n",
    "    return df_batch\n",
    "```\n",
    "\n",
    "However, note that if you transfer this logic to a lambda function, data will most probably have to be downloaded from another location, for example an S3 bucket. Loading logic for such a case could look like:\n",
    "def load_data()\n",
    "\n",
    "```python\n",
    "def load_csv_from_s3(\n",
    "    s3_bucket_client: boto3.resource(\"s3\").Bucket, file_key: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Function that loads a csv from S3 into a pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_bucket_client : boto3.resource\n",
    "        Instantiated s3 bucket client using boto3. Note that it should already\n",
    "        be pointing to the bucket from which you want to load objects\n",
    "    file_key : str\n",
    "        Key to the channel within the tile to be loaded\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The loaded data as pandas DataFrame\n",
    "    \"\"\"\n",
    "    s3_object = s3_bucket_client.Object(file_key).get()\n",
    "    df_batch = pd.read_csv(s3_object['Body'])\n",
    "\n",
    "    return df_batch\n",
    "```\n",
    "\n",
    "Pay special attention to what inputs this function needs and how it knows what file to load, since this will need to be used in the lambda as well. If you are able to pass the path to the file during runtime when using a lambda (e.g. in the event sent to it), you can simply use that as a parameter of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### PUT YOUR DATA LOADING LOGIC HERE\n",
    "# # To generate a testing suite, a batch of data can be used to develop\n",
    "# # validations. Accordingly, in the lines below custom logic is required to load\n",
    "# # a test dataset as a pandas DataFrame in the df_batch argument\n",
    "# # To make the RuntimeBatchRequest complete, which is used by GE for the\n",
    "# # validations, an asset_name and batch_identifier are required\n",
    "# df_batch = load_data()  # Needs to be defined!\n",
    "# batch_identifier = \"BATCH IDENTIFIER OR LOGIC TO GENERATE IT GOES HERE\"\n",
    "# asset_name = \"ASSET NAME OR LOGIC TO GENERATE IT GOES HERE\"\n",
    "\n",
    "# -- Get object keys from S3, sort, pick oldest\n",
    "list_objects = get_file_keys_from_s3(s3_client, test_config.data_bucket, PATH_TUTORIAL_DATA)\n",
    "list_objects.sort()\n",
    "asset_name = list_objects[0]\n",
    "\n",
    "# -- Load dataset\n",
    "path_batch_object = f\"s3://{test_config.data_bucket}/{asset_name}\"\n",
    "df_batch = load_data(path_batch_object)\n",
    "batch_identifier = \"tutorial_batch_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate batch request, generate suite and start validator\n",
    "When you have succesfully written logic to load data, the loaded batch will be passed to a RuntimeBatchRequest below in order to start an expectation suite. Great Expectations requires data to be passed as a request when you want to use the context to generate things such as expectation suites and checkpoints. The RunetimeBatchRequest is used here, because we are loading data (with our own logic) at runtime and want to pass that to subsequent objects.\n",
    "\n",
    "Using the RuntimeBatchRequest, a suite and validator are initiated and you can start writing expectations in the next cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3. Generate batch request at runtime using loaded tile\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"runtime_data\",\n",
    "    data_connector_name=\"runtime_data_connector\",\n",
    "    data_asset_name=asset_name,\n",
    "    runtime_parameters={\"batch_data\": df_batch},\n",
    "    batch_identifiers={\"batch_identifier\": batch_identifier,},\n",
    ")\n",
    "\n",
    "# -- 4. Generate suite, start validator\n",
    "suite = context.create_expectation_suite(\n",
    "    test_config.expectations_suite_name,\n",
    "    overwrite_existing=True,  \n",
    ")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=test_config.expectations_suite_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectations\n",
    "Using the validator object, expectations can be formulated below. Since Great Expectations comes with many expectations out of the box, [this page](https://greatexpectations.io/expectations) is generally a good place to start browsing through these. \n",
    "\n",
    "Predefined expectations can be used by calling them using the validator object and passing the required arguments. For example, to run an expectation on the number of rows in a dataframe, the following snippet can be used:\n",
    "\n",
    "```python\n",
    "# Get number of rows of current batch\n",
    "row_count = df_batch.shape[0]\n",
    "\n",
    "# Make expectation where the maximum deviation from the batch number of rows is 1%\n",
    "max_delta = 0.01\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-max_delta), max_value=row_count * (1+max_delta))\n",
    "```\n",
    "\n",
    "If you want to develop custom expectations, more information can be found about there [here](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Table level expectations\n",
    "\n",
    "# -- 1. Expect columns in table\n",
    "expected_columns = df_batch.columns\n",
    "validator.expect_table_columns_to_match_set(\n",
    "    column_set=expected_columns, exact_match=True\n",
    ")\n",
    "\n",
    "# -- 2. Expect row in between range, based on set delta\n",
    "row_count = df_batch.shape[0]\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-ROW_COUNT_DELTA), \n",
    "    max_value=row_count * (1+ROW_COUNT_DELTA),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Column level expectations\n",
    "\n",
    "# -- 1. Values are never null\n",
    "for column in expected_columns:\n",
    "    validator.expect_column_values_to_not_be_null(column)\n",
    "\n",
    "# -- 2. Date columns are parseable\n",
    "date_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "for column in date_columns:\n",
    "    validator.expect_column_values_to_be_dateutil_parseable(column)\n",
    "\n",
    "# -- 3. Check dtypes, assuming dtypes of the batch dataset are correct (this is\n",
    "#       something you might rather want to hard-code for real products)\n",
    "dict_dtypes = {}\n",
    "for column, dtype in zip(df_batch.dtypes.index, df_batch.dtypes):\n",
    "    dict_dtypes[column] = str(dtype)\n",
    "\n",
    "for column, dtype in dict_dtypes.items():\n",
    "    validator.expect_column_values_to_be_of_type(column, dtype)\n",
    "\n",
    "# -- 4. Expect values of specific columns to be between lower- and upper bounds\n",
    "dict_bounds = {\"VendorID\":[1,2],\n",
    "              \"payment_type\":[1,4]\n",
    "              }\n",
    "\n",
    "for column, (lower_bound, upper_bound) in dict_bounds.items():\n",
    "    # NOTE: for large datasets, this expectation is really slow. Using seperate tests\n",
    "    # for what the minimum should be and the maximum should be is a lot faster, while\n",
    "    # achieving the same test-wise\n",
    "    validator.expect_column_values_to_be_between(column, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dynamic validations\n",
    "test_column = \"passenger_count\"\n",
    "max_passenger_count = df_batch[test_column].max()\n",
    "\n",
    "validator.expect_column_max_to_be_between(\n",
    "    column=test_column,\n",
    "    min_value={\n",
    "        \"$PARAMETER\": \"min_max_passenger_count\",\n",
    "        \"$PARAMETER.min_max_passenger_count\": max_passenger_count\n",
    "        },\n",
    "    max_value={\n",
    "        \"$PARAMETER\": \"max_max_passenger_count\",\n",
    "        \"$PARAMETER.max_max_passenger_count\": max_passenger_count+2\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalize suite and create checkpoint\n",
    "After running all the expectations that you want to apply to the data, the cell below can be executed to save the set of expectations as a suite and couple it with a checkpoint. This checkpoint can be used in other scripts (lambda_function.py) by passing a new batch of data (as RuntimeBatchRequest) along with the checkpoint name to the `run_checkpoint` method of an initialized DataContext (which is the same as the `context` object initialized in this notebook. Such a call would look like:\n",
    "\n",
    "```python\n",
    "results = context.run_checkpoint(\n",
    "    checkpoint_name=\"CONFIG_NAME\",\n",
    "    validations=[{\"batch_request\": batch_request}],\n",
    "    )\n",
    "```\n",
    "\n",
    "**NOTE**: you should choose what kind of checkpoint you want to use below: one with or without automatic Data Docs updates\n",
    "\n",
    "When the code below is ran, Great Expectations automatically saves the expectation suite and checkpoint to S3 via the validator and context objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 6. Save suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "# -- 7. Create checkpoint\n",
    "#       Two options are provided here for creating a checkpoint: \n",
    "#       1. Use a SimpleCheckpoint, which contains an action to automatically update the\n",
    "#          Data Docs website whenever a validation is run\n",
    "#       2. Use checkpoint_without_datadocs_update, which generates the same\n",
    "#          SimpleCheckpoint but without an automatic Data Docs update action. This is\n",
    "#          useful if you run many validations (1000+) in parallel, since rendering the\n",
    "#          Data Docs website becomes very slow in that case          \n",
    "\n",
    "# -- 7.1 Create Simple checkpoint with automatic data docs updates (DEFAULT)\n",
    "checkpoint_config = {\n",
    "    \"name\": test_config.checkpoint_name,\n",
    "    \"config_version\": 3,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"expectation_suite_name\": test_config.expectations_suite_name,\n",
    "    \"run_name_template\": test_config.run_name_template,\n",
    "}\n",
    "\n",
    "# -- 7.2 Create checkpoint without automatic data docs update\n",
    "# checkpoint_config = checkpoint_without_datadocs_update(test_config)\n",
    "\n",
    "context.add_checkpoint(**checkpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Data Docs website\n",
    "Next, the command below can be ran to build the Data Docs website on S3, which provides an interactive user interface in which you can browse through expectation suites and check validation results. More on Data Docs can be found [here](https://docs.greatexpectations.io/docs/reference/data_docs/)\n",
    "\n",
    "In general, if you generate the website once, new validations should automatically be loaded onto it. If the website start lagging behind, just rerun the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_site_output = context.build_data_docs()\n",
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Initialize client\n",
    "lambda_client = boto3.client('lambda', region_name='eu-west-1')\n",
    "\n",
    "# -- Get object keys from S3, sort, pick oldest\n",
    "list_objects_lambda = list_objects[1:]\n",
    "\n",
    "# -- Invoke lambdas\n",
    "responses = tutorial_invoke_lambdas(lambda_client, list_objects_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Jupyter server\n",
    "After running all commands above and generating an expectation suite, checkpoint and website, run the command below to stop the Jupyter server from running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"jupyter notebook stop\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac66d493e7f5b77570bf12773b6869115396db7e1354e535321961352045d874"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('grater_expectations_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
